---
title: Interface
layout: default
parent: Assessment
nav_order: 3
---

# Inference Module Real-Time Evaluation

## 1. Description

This report presents the real-time performance analysis of the **Inference Engine** module in the FluxSand project. This module utilizes a pre-trained ONNX model to classify motion or status based on time-series sensor input.

The analysis is based on 50 consecutive inference runs as recorded in the unit test log. The model takes as input a 3D tensor of shape `[1, 1500, 8]` and produces output of shape `[-1, 10]`.

## 2. Test Environment

- **Model**: ONNX format, loaded from `/usr/local/share/FluxSand/model/model.onnx`
- **Input Shape**: [1, 1500, 8]
- **Output Shape**: [*, 10]
- **Framework**: ONNX Runtime
- **Platform**: ARM-based Linux system with RT kernel
- **Thread Affinity / Priority**: Not explicitly set in test, assumed normal user-space context

## 3. Inference Time Results (from 50 runs)

| Run # | Inference Time (ms) | Result         |
|------:|---------------------:|----------------|
| 1     | 14.979              | Unrecognized   |
| 2     | 14.202              | Unrecognized   |
| 3     | 14.242              | Still          |
| ...   | ...                 | ...            |
| 25    | 10.213              | Still          |
| 33    | 15.805              | Still          |
| 50    | 12.817              | Still          |

### Summary

- **Total Runs**: 50
- **Minimum Time**: 10.213 ms
- **Maximum Time**: 18.018 ms
- **Average Time**: 13.701 ms

## 4. Analysis

- The inference latency across 50 runs **ranged from ~10.2 ms to 18.0 ms**, which indicates relatively stable performance with slight variations likely due to thread scheduling or memory fluctuations.
- The **average latency is 13.701 ms**, allowing the system to handle inference at approximately **70–100 FPS** if inference is the only load.
- The early runs exhibited slightly higher latency, likely due to initial memory paging or cache warm-up. The majority of inferences settled within **12–15 ms**.
- The result output was dominantly "Still", indicating correct environment classification consistency once the model was stabilized.

## 5. Conclusion

The inference module demonstrates **stable and acceptable real-time performance**. With average response times under 14 ms, the system is capable of **responsive and interactive behavior**, suitable for continuous or event-driven AI inference tasks in embedded environments.
